{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def normalize(state, env):\n",
    "    return (state-env.state_space_low)/(env.state_space_high-env.state_space_low)\n",
    "\n",
    "\n",
    "def de_normalize(state, env):\n",
    "    return state*(env.state_space_high-env.state_space_low)+env.state_space_low\n",
    "\n",
    "# Define the Q-learning agent class\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, gamma, epsilon, epsilon_decay, alpha_decay, get_state, action_space):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.get_state = get_state\n",
    "        self.action_space = action_space\n",
    "        self.num_actions = len(action_space)\n",
    "\n",
    "        \n",
    "        self.v_get_Q_value = np.vectorize(self.get_Q_value)\n",
    "        self.v_get_next_action = np.vectorize(self.get_next_action)\n",
    "\n",
    "    def gen_Q_table(self, action_space, states):\n",
    "        \n",
    "        self.Q = pd.DataFrame(np.random.random((len(action_space), len(states)))*(-0.1),\n",
    "                            columns=states,\n",
    "                            index=action_space)\n",
    "\n",
    "    def get_Q_value(self, state, action):\n",
    "        if state is None:\n",
    "            return 0.0\n",
    "        \n",
    "        if state in self.Q.columns and action in self.Q.index:\n",
    "            return self.Q[state][action]\n",
    "        \n",
    "        return 0.0\n",
    "\n",
    "    def get_next_action(self, max_Q_value, Q_values):\n",
    "        if Q_values == max_Q_value:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def choose_action(self, state, greedy=False):\n",
    "\n",
    "        if greedy and random.random() < self.epsilon:\n",
    "            action = random.choice(self.action_space)\n",
    "        else:\n",
    "           \n",
    "            Q_values = self.v_get_Q_value(state, self.action_space)\n",
    "\n",
    "            max_Q_value = max(Q_values)\n",
    "          \n",
    "            #action = random.choice([action for i, action in enumerate(self.action_space) if Q_values[i] == max_Q_value])\n",
    "            action = self.action_space[random.choice(np.where(self.v_get_next_action(max_Q_value, Q_values))[0])]\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def update_Q_value(self, state, action, reward):\n",
    "\n",
    "        #Q_next = [self.get_Q_value(next_state, next_action) for next_action in self.action_space]\n",
    "        \n",
    "        Q_next = self.v_get_Q_value(state, self.action_space)\n",
    "\n",
    "        max_Q_next = max(Q_next) if len(Q_next) else 0.0\n",
    "        current_Q = self.get_Q_value(state, action)\n",
    "        self.Q[state][action] = current_Q + self.alpha * (reward + self.gamma * max_Q_next - current_Q)\n",
    "\n",
    "    def test(self, num_episodes, num_steps, sp_steps):\n",
    "        del_next_state = 0\n",
    "        action = 0\n",
    "        state = 0\n",
    "        state_space_high = 40\n",
    "        state_space_low = -10\n",
    "        LOG_DIR = 'logs'\n",
    "        colors = mcolors.TABLEAU_COLORS\n",
    "\n",
    "\n",
    "        env = Environment()\n",
    "        \n",
    "        sp_list = [np.random.random()*state_space_high+state_space_low for i in range(num_steps//sp_steps)]\n",
    "        states = np.zeros((num_steps, num_episodes))\n",
    "        actions = np.zeros((num_steps, num_episodes))\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "\n",
    "            states = np.zeros((num_steps, num_episodes))\n",
    "            actions = np.zeros((num_steps, num_episodes))\n",
    "            del_next_state = env.reset()\n",
    "            state = 0\n",
    "            action = 0\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                SP = sp_list[step//(sp_steps+1)]\n",
    "\n",
    "                del_action = agent.choose_action(del_next_state, greedy=True)\n",
    "                action += del_action\n",
    "                action = np.round(np.clip(action,env.action_space_low, env.action_space_high), 1)\n",
    "\n",
    "                del_next_state, reward = env.step(action, SP)\n",
    "                state += del_next_state\n",
    "                state = np.round(np.clip(state, state_space_low, state_space_high), 1)\n",
    "                \n",
    "                states[step][episode] = state\n",
    "                actions[step][episode] = action\n",
    "\n",
    "            if del_next_state is None:\n",
    "                print('del_next_state None')\n",
    "                if state-(env.state_space_low+env.state_space_high)/2 < 0:\n",
    "                    del_next_state = env.state_space_low\n",
    "                else:\n",
    "                    del_next_state = env.state_space_high\n",
    "\n",
    "        \n",
    "        fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(12, 7))\n",
    "        states = states.flatten()\n",
    "        actions = actions.flatten()\n",
    "        ax1.plot(states, colors['tab:blue'])\n",
    "        ax1.plot([sp_list[i//sp_steps] for i in range(num_steps)]*num_episodes, colors['tab:orange'])\n",
    "        ax2.plot(actions, colors['tab:green'])\n",
    "        \n",
    "        ax1.set(ylabel=\"SetPoint and state (kPa)\", title=\"Trajectory\")\n",
    "        ax2.set(ylabel=\"Action u(t) /rpm\", xlabel=\"Steps\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, alpha_u=0.1, reward_scale=0.1, *args, **kwargs):\n",
    "        self.states = np.round(np.arange(-5, 5, 0.1), 2)\n",
    "        self.state_space_low = self.states[0]\n",
    "        self.state_space_high = self.states[-1]\n",
    "\n",
    "        self.action_space = np.round(np.arange(-5, 5, 0.1), 2)\n",
    "        self.action_space_low = 0\n",
    "        self.action_space_high = 50\n",
    "\n",
    "        self.num_actions = 1\n",
    "        self.num_states = 1\n",
    "        self.reward = 0.0\n",
    "        self.state = 0.0\n",
    "        self.action = 0.0\n",
    "        self.alpha_u = alpha_u\n",
    "        self.reward_scale = reward_scale\n",
    "        self.SP = []\n",
    "\n",
    "    def get_next_state(self, u):\n",
    "        x = -2.1761 + -0.202*self.state + 0.014*u**2\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.reward = 0.0\n",
    "        self.state = 0.0\n",
    "        self.action = 0.0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, sp):\n",
    "        \n",
    "        # clip state space and calculate reward\n",
    "        \n",
    "        del_next_state = np.round(np.clip(self.get_next_state(action), self.state_space_low, self.state_space_high), 1)\n",
    "        \n",
    "        # get reward for un-normalized state and action as step is a function for un-normalized state and action\n",
    "        self.reward = -self.reward_scale*(np.abs(sp-self.state) + self.alpha_u*np.abs(self.action-action))\n",
    "        #print('state',self.state, 'action', action, 'next_state', next_state)\n",
    "        \n",
    "        self.state += del_next_state\n",
    "        self.action = np.round(action, 1)\n",
    "\n",
    "        # return normalized state\n",
    "        return del_next_state, self.reward\n",
    "\n",
    "\n",
    "# Define the main function\n",
    "def Q_run():\n",
    "\n",
    "    alpha = 0.133 #trial.suggest_loguniform('alpha', 0.0001, 1)\n",
    "    gamma = 0.99 #trial.suggest_loguniform('gamma', 0.0001, 1)\n",
    "    agent = QLearningAgent(alpha=alpha, gamma=gamma, epsilon=0.6, epsilon_decay=3/num_episodes,\n",
    "                        alpha_decay=2/num_episodes, get_state=lambda: 0, action_space=env.action_space)\n",
    "    agent.gen_Q_table(agent.action_space, env.states)\n",
    "\n",
    "\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        step = 0\n",
    "        action = 0\n",
    "        del_next_state = 0\n",
    "\n",
    "        while state is not None and step < max_steps:\n",
    "            SP = sp_list[step//(sp_steps+1)]\n",
    "\n",
    "            del_action = agent.choose_action(del_next_state)\n",
    "            action += del_action\n",
    "            action = np.round(np.clip(action,0, 50), 1)\n",
    "            \n",
    "\n",
    "            del_next_state, reward = env.step(action, SP)\n",
    "            state += del_next_state\n",
    "            state = np.round(np.clip(state, -10, 40), 1)\n",
    "            rewards[episode] += reward\n",
    "           \n",
    "            \n",
    "            if episode%50 == 0:\n",
    "              print(f'step: {step:5d}, state:{state:4.2f}, del_next_state:{del_next_state:4.2f}, action: {action:4.2f}')\n",
    "              with summary_writer.as_default():\n",
    "                tf.summary.scalar('trajectory', state, step=step+max_steps*episode//50)\n",
    "              with summary_writer2.as_default():\n",
    "                tf.summary.scalar('trajectory', SP, step=step+max_steps*episode//50)\n",
    "                tf.summary.scalar('action', action, step=step+max_steps*episode//50)\n",
    "              pass\n",
    "\n",
    "            if del_next_state is None:\n",
    "                print('del_next_state None')\n",
    "                if state-(env.state_space_low+env.state_space_high)/2 < 0:\n",
    "                    del_next_state = env.state_space_low\n",
    "                else:\n",
    "                    del_next_state = env.state_space_high\n",
    "                 \n",
    "            else:\n",
    "                # continue the episode\n",
    "                error[episode] += np.abs(SP-state)\n",
    "            agent.update_Q_value(del_next_state, del_action, reward)\n",
    "\n",
    "            step += 1\n",
    "        # epsilon decay & alpha decay\n",
    "        agent.epsilon = agent.epsilon*np.exp(-agent.epsilon_decay)\n",
    "        agent.alpha = agent.alpha*np.exp(-agent.alpha_decay)\n",
    "        \n",
    "        if episode%10 == 0:\n",
    "            print(f\"Episode: {episode : 04d}/{num_episodes : 04d} | episodic reward: {rewards[episode]: 04.2f} |  error:{error[episode]: 04.2f}\")\n",
    "        \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('error', error[episode], step=episode)\n",
    "                tf.summary.scalar('reward', rewards[episode], step=episode)\n",
    "                \n",
    "                a = tf.convert_to_tensor(agent.Q)\n",
    "                img = np.reshape((a - tf.math.reduce_min(a))/(tf.math.reduce_max(a)-tf.math.reduce_min(a)), (-1, agent.Q.shape[0], agent.Q.shape[1], 1))\n",
    "                tf.summary.image('Q table', img, step=episode)\n",
    "                \n",
    "\n",
    "        \n",
    "    return agent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/2000, episodic reward: -148275.88, set point: 10.4, error:243.46000000000006\n",
      "Episode: 101/2000, episodic reward: -120903.19, set point: -0.5, error:385.03\n",
      "Episode: 201/2000, episodic reward: -96306.69, set point: 2.3, error:527.47\n",
      "Episode: 301/2000, episodic reward: -182641.52, set point: 14.6, error:-89.3299999999999\n",
      "Episode: 401/2000, episodic reward: -104485.29, set point: 6.4, error:492.51\n",
      "Episode: 501/2000, episodic reward: -259163.17, set point: 18.6, error:73.27000000000004\n",
      "Episode: 601/2000, episodic reward: -98213.19, set point: 7.1, error:-260.71\n",
      "Episode: 701/2000, episodic reward: -96722.23, set point: 1.5, error:1102.99\n",
      "Episode: 801/2000, episodic reward: -184366.32, set point: 15.6, error:-893.4499999999998\n",
      "Episode: 901/2000, episodic reward: -85827.26, set point: 1.7, error:824.8299999999999\n",
      "Episode: 1001/2000, episodic reward: -86680.7, set point: 4.8, error:1389.08\n",
      "Episode: 1101/2000, episodic reward: -102413.67, set point: 5.8, error:-1497.0\n",
      "Episode: 1201/2000, episodic reward: -134580.36, set point: 10.9, error:1261.5\n",
      "Episode: 1301/2000, episodic reward: -146588.5, set point: 0.4, error:572.7500000000001\n",
      "Episode: 1401/2000, episodic reward: -104136.68, set point: 6.2, error:-756.9399999999998\n",
      "Episode: 1501/2000, episodic reward: -223269.04, set point: 17.9, error:-589.38\n",
      "Episode: 1601/2000, episodic reward: -86051.13, set point: 9.9, error:25.50000000000074\n",
      "Episode: 1701/2000, episodic reward: -112517.39, set point: 11.6, error:693.0600000000002\n",
      "Episode: 1801/2000, episodic reward: -99298.98, set point: 5.1, error:729.9100000000001\n",
      "Episode: 1901/2000, episodic reward: -123219.82, set point: 11.5, error:733.94\n",
      "1096.4422872000014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = Environment()\n",
    "\n",
    "# run the Q-learning algorithm for a fixed number of episodes\n",
    "num_episodes = 10000\n",
    "max_steps = 500\n",
    "sp_steps = 50\n",
    "alpha = 0\n",
    "gamma = 0\n",
    "\n",
    "log_dir = os.path.join('logs', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "if not os.path.exists('logs'):\n",
    "    os.mkdir('logs')\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "summary_writer2 = tf.summary.create_file_writer(log_dir+\"_SP\")\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "error = np.zeros(num_episodes)\n",
    "sp_list = [np.random.random()*35-5 for i in range(max_steps//sp_steps)]\n",
    "#[20]*(max_steps//sp_steps)#\n",
    "print(\"SP List: \",sp_list)\n",
    "\n",
    "agent = Q_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb9b12ace943f3bd123231c95778ee62e6cf3a3a38188c92df7ee0e2d5c130e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
